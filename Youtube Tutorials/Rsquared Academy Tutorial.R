"The link to the video:
https://www.youtube.com/watch?v=l37n_HDD1qs&list=WL&index=4&t=880s"

library(rvest)
library(robotstxt)
library(selectr)
library(xml2)
library(dplyr)
library(stringr)
library(forcats)
library(magrittr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(tibble)
library(purrr)

#robotstxt is used to check whether we can scrape the website or not.
#xml2 is used to download the webpage as a xml document

link_imdb <- "https://www.imdb.com/search/title?groups=top_250&sort=user_rating"
paths_allowed(link_imdb)

"The paths_allowed function returns a boolean value given a URL. If it returns
TRUE then the link is okay to be scraped, but if it returns false then it is
not okay."

#Checking various other websites
link_instagram <- "https://www.instagram.com/explore/"
paths_allowed(link_instagram)

"Lol, instagram is not okay to be scraped."

link_amazon <- "https://www.amazon.com/ref=nav_logo"
paths_allowed(link_amazon)

"Amazon is okay to be scraped!"

link_linkedin_home <- "https://www.linkedin.com/feed/"
paths_allowed(link_linkedin_home)

"Linkedin is not okay to be scraped. As far as i know linkedin lost a court
order to prevent their website to be scraped. It seems that paths_allowed only
tells us whether the owner of the website feels it is okay for their website
to be scraped or not. In reality every website is getting scraped everyday.
Including linkedin and instagram."

link_tokopedia_home <- "https://www.tokopedia.com/"
paths_allowed(link_tokopedia_home)

"Oh Tokopedia is okay to be scraped."
link_bukalapak_home <- "https://www.bukalapak.com/"
paths_allowed(link_bukalapak_home)

"Bukalapak is also okay. Okay back to the course."
rm(link_amazon, link_bukalapak_home, link_instagram,
   link_linkedin_home, link_tokopedia_home)

#Reading the IMDB page
page_imdb <- read_html(link_imdb)
class(page_imdb)

"We successfully read the page using the function read_html. This is a function
from the xml2 library. We can see that the class of the object is xml document."

#Grabbing the name of the movies
movie_names <- page_imdb %>%
                html_nodes(".lister-item-header a") %>%
                html_text()

"It is interesting that these commands also work. These commands were generated
by looking at the actual structure of the HTML file. The previous one was
generated by the SelectorGadget Chrome extension."

page_imdb %>%
  html_nodes(".lister-item-image img") %>%
  html_attr("alt")

#Grabbing their year of publication
movie_year <- page_imdb %>%
                html_nodes(".text-muted.unbold") %>%
                html_text()

"Note: In the video the instructor was mentioning that he will demonstrate 4
case studies but the video includes only 1 and it is not even complete. He did
not use some of the libraries he loaded. I wonder if there is a part 2 to the
video.

I looked at the channel page and couldn't find any sequel to this video. The
script in the github page is also very short, It only includes 2 examples,
Wikipedia and IMDB. The github script does not even give the same link as the
video. In the video he uses Amazon for demonstration.

This is very confusing. Judging from the fact that the video was made 2 years
ago (I watched it on 2021-05-18 12:32:21 +07) I guess there will be no sequel
at all."

"It is fine, i will look for other tutorials."